{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goyal26sachin/Spoon-Knife/blob/master/Momentum_Strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hKcs2fgbI2b"
      },
      "outputs": [],
      "source": [
        "import os, sys, time, math\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# optional - install yfinance when running in Colab or new env\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except Exception:\n",
        "    print(\"yfinance not installed. Install with: pip install yfinance\")\n",
        "    # raise or continue; user should install\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG / Defaults\n",
        "# -------------------------\n",
        "DEFAULT_INPUT_TICKER_XLSX = \"/content/Input_List_of_Stocks.xlsx\"     # <---- \"/mnt/data/BSE-500.xlsx\"   # <--- change to /content/BSE-500.xlsx in Colab if needed\n",
        "OUTPUT_DIR = Path(\"./output\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Helper utilities\n",
        "# -------------------------\n",
        "def confirm(prompt=\"Proceed? (y/n): \"):\n",
        "    ans = input(prompt).strip().lower()\n",
        "    return ans in (\"y\",\"yes\")\n",
        "\n",
        "def safe_save_df_to_excel(writer, df, sheet_name):\n",
        "    # Excel sheet name limit 31 chars\n",
        "    name = str(sheet_name)[:31]\n",
        "    try:\n",
        "        df.to_excel(writer, sheet_name=name, index=False)\n",
        "    except Exception as e:\n",
        "        # fallback: truncate further\n",
        "        df.to_excel(writer, sheet_name=name[:25], index=False)\n",
        "\n",
        "# -------------------------\n",
        "# Module 1: Data downloader\n",
        "# -------------------------\n",
        "def download_yahoo_data(ticker_list_file=DEFAULT_INPUT_TICKER_XLSX,\n",
        "                        output_path=OUTPUT_DIR / \"yahoo_data.xlsx\",\n",
        "                        verbose=True,\n",
        "                        append_suffix=\".NS\",\n",
        "                        years=3): # Added years parameter with default 3\n",
        "    \"\"\"\n",
        "    Reads tickers from input XLSX (first sheet, first column or 'Ticker'/'Symbol' col).\n",
        "    Downloads OHLCV + Adj Close and basic info (trailingPE/forwardPE) for each ticker via yfinance.\n",
        "    Saves per-symbol sheet + Combined_Data + PE_Data + Errors to output_path.\n",
        "    Returns path to saved file.\n",
        "    \"\"\"\n",
        "    print(\"Reading ticker list from:\", ticker_list_file)\n",
        "    xlf = pd.ExcelFile(ticker_list_file)\n",
        "    sheet = xlf.sheet_names[0]\n",
        "    df_list = pd.read_excel(ticker_list_file, sheet_name=sheet)\n",
        "    # detect ticker column\n",
        "    ticker_col = None\n",
        "    for c in df_list.columns:\n",
        "        if str(c).strip().lower() in (\"ticker\",\"symbol\",\"code\"):\n",
        "            ticker_col = c; break\n",
        "    if ticker_col is None:\n",
        "        ticker_col = df_list.columns[0]\n",
        "    raw_tickers = df_list[ticker_col].astype(str).str.strip().dropna().unique().tolist()\n",
        "    # Automatically append .NS if not present\n",
        "    tickers = [t if \".\" in t else t + append_suffix for t in raw_tickers if t and str(t).upper() not in (\"NAN\",\"NONE\",\"\")]\n",
        "    print(f\"Found {len(tickers)} tickers (sample):\", tickers[:10])\n",
        "    sheets = {}\n",
        "    pe_records = []\n",
        "    errors = []\n",
        "    for i, sym in enumerate(tickers, start=1):\n",
        "        try:\n",
        "            print(f\"[{i}/{len(tickers)}] Downloading {sym} ...\", end=\" \")\n",
        "            ticker = yf.Ticker(sym)\n",
        "            hist = ticker.history(period=f\"{years}y\", auto_adjust=False) # Used years parameter\n",
        "            if hist is None or hist.shape[0]==0:\n",
        "                hist = ticker.history(period=f\"{years}y\", auto_adjust=False) # Used years parameter\n",
        "            if hist is None or hist.shape[0]==0:\n",
        "                print(\"NO DATA\")\n",
        "                errors.append((sym,\"no_history\"))\n",
        "                continue\n",
        "            df_hist = hist.reset_index().rename(columns={'Date':'Date'})\n",
        "\n",
        "            # Prioritize 'Adj Close', if not available use 'Close'\n",
        "            price_col = 'Adj Close' if 'Adj Close' in df_hist.columns else ('Close' if 'Close' in df_hist.columns else None)\n",
        "\n",
        "            if price_col is None:\n",
        "                print(\"No price data found (Adj Close or Close)\")\n",
        "                errors.append((sym, \"no_price_data\"))\n",
        "                continue\n",
        "\n",
        "            df_hist = df_hist[['Date', price_col, 'Volume']].copy() # Added 'Volume' here\n",
        "            df_hist.rename(columns={price_col: 'Adj Close'}, inplace=True) # Standardize column name to 'Adj Close'\n",
        "\n",
        "            # Convert 'Date' column to be timezone-naive if it's timezone-aware\n",
        "            if 'Date' in df_hist.columns and isinstance(df_hist['Date'].dtype, pd.DatetimeTZDtype):\n",
        "                df_hist['Date'] = df_hist['Date'].dt.tz_localize(None)\n",
        "\n",
        "            df_hist[\"Symbol\"] = sym\n",
        "            sheets[sym] = df_hist\n",
        "            info = ticker.info if hasattr(ticker,'info') else {}\n",
        "            pe_records.append({\"Symbol\": sym, \"TrailingPE\": info.get(\"trailingPE\", None), \"ForwardPE\": info.get(\"forwardPE\", None)})\n",
        "            print(f\"done ({df_hist.shape[0]} rows)\")\n",
        "        except Exception as e:\n",
        "            print(\"error:\", e)\n",
        "            errors.append((sym,str(e)))\n",
        "    # save to excel\n",
        "    output_path = Path(output_path)\n",
        "    with pd.ExcelWriter(output_path, engine=\"openpyxl\", datetime_format=\"YYYY-MM-DD\") as writer:\n",
        "        for sym, df in sheets.items():\n",
        "            safe_save_df_to_excel(writer, df, sym[:31])\n",
        "        if len(sheets)>0:\n",
        "            combined = pd.concat(sheets.values(), ignore_index=True, sort=False)\n",
        "            safe_save_df_to_excel(writer, combined, \"Combined_Data\")\n",
        "        safe_save_df_to_excel(writer, pd.DataFrame(pe_records), \"PE_Data\")\n",
        "        safe_save_df_to_excel(writer, pd.DataFrame(errors, columns=[\"Symbol\",\"Error\"]), \"Errors\")\n",
        "    print(\"Saved downloader output to:\", output_path)\n",
        "    return output_path\n",
        "\n",
        "# -------------------------\n",
        "# Module 2: Data cleanup\n",
        "# -------------------------\n",
        "def clean_data(yahoo_xlsx_path, output_clean_path=OUTPUT_DIR/\"cleaned_price_matrix.xlsx\", min_history_days=260, pct_missing_threshold=0.2):\n",
        "    \"\"\"\n",
        "    Loads the 'Combined_Data' sheet or builds a wide price matrix from per-symbol sheets.\n",
        "    Outputs:\n",
        "       - Price matrix (wide)\n",
        "       - Per-symbol summary with pct missing, zeros, spikes, stale runs, first/last valid date\n",
        "    Also saves cleaned price matrix with only valid symbols (>= min_history_days non-null)\n",
        "    Returns (clean_price_df, summary_df)\n",
        "    \"\"\"\n",
        "    print(\"Loading yahoo data file:\", yahoo_xlsx_path)\n",
        "    xl = pd.ExcelFile(yahoo_xlsx_path)\n",
        "    if \"Combined_Data\" in xl.sheet_names:\n",
        "        df = pd.read_excel(yahoo_xlsx_path, sheet_name=\"Combined_Data\", parse_dates=[\"Date\"])\n",
        "    else:\n",
        "        # reconstruct combined from first sheet's symbol sheets\n",
        "        # load every sheet and append\n",
        "        frames = []\n",
        "        for s in xl.sheet_names:\n",
        "            if s in (\"Combined_Data\",\"PE_Data\",\"Errors\"): continue\n",
        "            tmp = pd.read_excel(yahoo_xlsx_path, sheet_name=s, parse_dates=[\"Date\"])\n",
        "            if \"Symbol\" not in tmp.columns:\n",
        "                tmp[\"Symbol\"] = s\n",
        "            frames.append(tmp)\n",
        "        df = pd.concat(frames, ignore_index=True)\n",
        "    # pivot to price matrix wide using Adj Close (preferred) else Close\n",
        "    df['Adj Close'] = df.get('Adj Close', df.get('AdjClose', np.nan))\n",
        "    if df['Adj Close'].notna().sum() > 0:\n",
        "        price_series = df.pivot(index=\"Date\", columns=\"Symbol\", values=\"Adj Close\")\n",
        "    else:\n",
        "        price_series = df.pivot(index=\"Date\", columns=\"Symbol\", values=\"Close\")\n",
        "    price_series = price_series.sort_index()\n",
        "    # compute per-symbol metrics\n",
        "    report = []\n",
        "    total_days = price_series.shape[0]\n",
        "    for sym in price_series.columns:\n",
        "        ser = pd.to_numeric(price_series[sym], errors='coerce')\n",
        "        non_null = ser.notna().sum()\n",
        "        pct_missing = 1 - (non_null / total_days) if total_days>0 else 1.0\n",
        "        zeros = (ser == 0).sum()\n",
        "        negatives = (ser < 0).sum()\n",
        "        unique_vals = ser.nunique(dropna=True)\n",
        "        first_valid = ser.first_valid_index()\n",
        "        last_valid = ser.last_valid_index()\n",
        "        min_p = ser.min(skipna=True)\n",
        "        max_p = ser.max(skipna=True)\n",
        "        pct = ser.pct_change(fill_method=None).replace([np.inf,-np.inf], np.nan) # Added fill_method=None\n",
        "        max_abs = float(np.nanmax(np.abs(pct.values))) if pct.notna().any() else np.nan\n",
        "        days_over_50 = int((np.abs(pct) > 0.5).sum())\n",
        "        # stale runs\n",
        "        longest_stale = 0\n",
        "        prev = None\n",
        "        run = 0\n",
        "        for v in ser.values:\n",
        "            if pd.isna(v):\n",
        "                prev = None; run = 0; continue\n",
        "            if prev is None or v != prev:\n",
        "                run = 1; prev = v\n",
        "            else:\n",
        "                run += 1\n",
        "            longest_stale = max(longest_stale, run)\n",
        "        report.append({\n",
        "            \"Symbol\": sym,\n",
        "            \"NonNullDays\": int(non_null),\n",
        "            \"PctMissing\": round(100*pct_missing,2),\n",
        "            \"Zeros\": int(zeros),\n",
        "            \"Negatives\": int(negatives),\n",
        "            \"UniqueValues\": int(unique_vals),\n",
        "            \"FirstValidDate\": str(first_valid) if first_valid is not None else \"\",\n",
        "            \"LastValidDate\": str(last_valid) if last_valid is not None else \"\",\n",
        "            \"MinPrice\": float(min_p) if not pd.isna(min_p) else np.nan,\n",
        "            \"MaxPrice\": float(max_p) if not pd.isna(max_p) else np.nan,\n",
        "            \"MaxAbsDailyReturn\": max_abs,\n",
        "            \"Days>50%Move\": days_over_50,\n",
        "            \"LongestStaleRun\": int(longest_stale)\n",
        "        })\n",
        "    report_df = pd.DataFrame(report).sort_values([\"PctMissing\",\"NonNullDays\"], ascending=[False,True])\n",
        "    # filter clean symbols\n",
        "    clean_symbols = report_df[report_df[\"NonNullDays\"] >= min_history_days][\"Symbol\"].tolist()\n",
        "    # also drop symbols with > pct_missing_threshold missing:\n",
        "    clean_symbols = [s for s in clean_symbols if report_df[report_df[\"Symbol\"]==s][\"PctMissing\"].iloc[0] <= 100*pct_missing_threshold]\n",
        "    # NEW: Remove stocks with Days>50%Move > 0 from clean_symbols\n",
        "    symbols_to_remove_due_to_spikes = report_df[report_df[\"Days>50%Move\"] > 0][\"Symbol\"].tolist()\n",
        "    clean_symbols = [s for s in clean_symbols if s not in symbols_to_remove_due_to_spikes]\n",
        "\n",
        "    clean_price_df = price_series[clean_symbols].copy()\n",
        "    # Save outputs\n",
        "    out_file = Path(output_clean_path)\n",
        "    with pd.ExcelWriter(out_file, engine=\"openpyxl\", datetime_format=\"YYYY-MM-DD\") as writer:\n",
        "        price_series.reset_index().to_excel(writer, sheet_name=\"RawPriceMatrix\", index=False)\n",
        "        clean_price_df.reset_index().to_excel(writer, sheet_name=\"CleanPriceMatrix\", index=False)\n",
        "        report_df.to_excel(writer, sheet_name=\"Symbol_Summary\", index=False)\n",
        "        report_df[report_df[\"PctMissing\"]>100*pct_missing_threshold].to_excel(writer, sheet_name=\"High_Missing\", index=False)\n",
        "        report_df[report_df[\"Days>50%Move\"]>0].to_excel(writer, sheet_name=\"Extreme_Spikes\", index=False)\n",
        "    print(\"Saved cleaned price matrix and summary to:\", out_file)\n",
        "    return out_file, clean_price_df, report_df\n",
        "\n",
        "# -------------------------\n",
        "# Module 3: Momentum engines\n",
        "# -------------------------\n",
        "def get_rebalance_dates(price_index, freq='fortnightly'):\n",
        "    \"\"\"price_index is a DatetimeIndex sorted ascending (daily trading days).\n",
        "    freq: 'weekly' (every Friday), 'fortnightly' (every other Friday), 'monthly' (last trading day of month).\n",
        "    Returns list of pd.Timestamp objects for rebalance days (subset of index).\n",
        "    \"\"\"\n",
        "    dates = pd.Series(sorted(price_index.unique()))\n",
        "    if freq == 'monthly':\n",
        "        rb = dates.groupby(dates.dt.to_period('M')).max().tolist()\n",
        "    elif freq == 'weekly':\n",
        "        fridays = dates[dates.dt.weekday == 4].tolist()\n",
        "        rb = fridays\n",
        "    elif freq == 'fortnightly':\n",
        "        fridays = dates[dates.dt.weekday == 4].tolist()\n",
        "        rb = fridays[::2] if len(fridays) >= 2 else dates[13::14].tolist()\n",
        "    else:\n",
        "        raise ValueError(\"freq must be one of weekly/fortnightly/monthly\")\n",
        "    return rb\n",
        "\n",
        "def compute_scores_and_signals(price_df, rebalance_dates):\n",
        "    \"\"\"\n",
        "    price_df: DataFrame indexed by Date with symbols as columns (Adj-close adjusted).\n",
        "    rebalance_dates: list of dates to compute signals on.\n",
        "    Returns signals_df with columns: RebalanceDate, Symbol, Price, R52_ex1, R26, R13, R4, Score, DMA50, DMA200\n",
        "    \"\"\"\n",
        "    signals = []\n",
        "    symbols = price_df.columns.tolist()\n",
        "    # convert to numpy arrays per symbol to speed calculations\n",
        "    for sym in symbols:\n",
        "        ser = pd.to_numeric(price_df[sym], errors='coerce').dropna()\n",
        "        if ser.shape[0] < 20:  # insufficient\n",
        "            continue\n",
        "        dates_idx = ser.index.values.astype('datetime64[ns]')\n",
        "        prices_arr = ser.values.astype(float)\n",
        "        # precompute DMAs using pandas on ser\n",
        "        s_pd = ser\n",
        "        dma50 = s_pd.rolling(window=50, min_periods=10).mean()\n",
        "        dma200 = s_pd.rolling(window=200, min_periods=30).mean()\n",
        "        for dt in rebalance_dates:\n",
        "            if dt < s_pd.index.min() or dt > s_pd.index.max():\n",
        "                continue\n",
        "            # helper to compute cumulative return by N observations\n",
        "            def cumret_by_n(n, exclude_last=0):\n",
        "                pos = np.searchsorted(dates_idx, np.datetime64(dt), side='right') - 1\n",
        "                if pos - exclude_last - n < 0 or pos < 0:\n",
        "                    return np.nan\n",
        "                start = pos - exclude_last - n\n",
        "                start_price = prices_arr[start]\n",
        "                end_price = prices_arr[pos - exclude_last]\n",
        "                if start_price == 0 or np.isnan(start_price) or np.isnan(end_price): return np.nan\n",
        "                return end_price / start_price - 1\n",
        "            R52 = cumret_by_n(260, exclude_last=5)\n",
        "            R26 = cumret_by_n(130, exclude_last=0)\n",
        "            R13 = cumret_by_n(65, exclude_last=0)\n",
        "            R4  = cumret_by_n(20, exclude_last=0)\n",
        "            pos = np.searchsorted(dates_idx, np.datetime64(dt), side='right') - 1\n",
        "            price = float(prices_arr[pos]) if pos>=0 and not np.isnan(prices_arr[pos]) else np.nan\n",
        "            # DMA values at pos (get via dma series)\n",
        "            dma50_v = float(dma50.iloc[pos]) if pos>=0 and not pd.isna(dma50.iloc[pos]) else np.nan\n",
        "            dma200_v = float(dma200.iloc[pos]) if pos>=0 and not pd.isna(dma200.iloc[pos]) else np.nan\n",
        "            # score\n",
        "            if all(pd.isna([R52,R26,R13,R4])):\n",
        "                score = np.nan\n",
        "            else:\n",
        "                score = 0.5*(R52 if not pd.isna(R52) else 0) + 0.25*(R26 if not pd.isna(R26) else 0) + 0.15*(R13 if not pd.isna(R13) else 0) + 0.10*(R4 if not pd.isna(R4) else 0)\n",
        "            signals.append({'RebalanceDate': pd.Timestamp(dt),'Symbol': sym, 'Price': price,\n",
        "                            'DMA50': dma50_v, 'DMA200': dma200_v, 'R52_ex1': R52, 'R26': R26, 'R13': R13, 'R4': R4, 'Score': score})\n",
        "    signals_df = pd.DataFrame(signals)\n",
        "    signals_df = signals_df.dropna(subset=['Score']).sort_values(['RebalanceDate','Score'], ascending=[True, False]).reset_index(drop=True)\n",
        "\n",
        "    # Drop rows where DMA50 or DMA200 are NaN, as these cannot be used for eligibility\n",
        "    signals_df = signals_df.dropna(subset=['DMA50', 'DMA200'])\n",
        "\n",
        "    # apply DMA eligibility flag / Conciously DMA 50 has been multiplied by .9.\n",
        "    # So that immediately if stock crosses below DMA50 it is not rejected.\n",
        "    signals_df['Eligible'] = (signals_df['Price'] > 0.9*signals_df['DMA50']) & (signals_df['Price'] > signals_df['DMA200']) & (signals_df['DMA50'] > signals_df['DMA200'])\n",
        "    return signals_df\n",
        "\n",
        "# -------------------------\n",
        "# Module 4: Backtest and reporting\n",
        "# -------------------------\n",
        "def backtest_grid_and_report(price_df, signals_df, rebalance_dates, freq_label, topN_list=(5,10,15,20), excludeK_list=(0,1,2,3), weighting='equal', out_xlsx=OUTPUT_DIR/\"results_fortnightly.xlsx\"):\n",
        "    \"\"\"\n",
        "    Run grid backtests on signals_df and price_df for given rebalance_dates,\n",
        "    compute metrics, and produce an Excel workbook with:\n",
        "      - Results grid\n",
        "      - Top strategies (top 10 by Sharpe)\n",
        "      - For each selected top strategy: holdings per rebalance (Symbol, Weight, Price_in, Price_out, Return)\n",
        "    \"\"\"\n",
        "    print(\"Running backtest grid for\", freq_label)\n",
        "    dates_sorted = sorted(rebalance_dates)\n",
        "    next_map = {dates_sorted[i]: (dates_sorted[i+1] if i < len(dates_sorted)-1 else None) for i in range(len(dates_sorted))}\n",
        "    rows = []\n",
        "    holdings_store = {}  # key (topN,excl) -> DataFrame holdings\n",
        "    periods_store = {}\n",
        "    for topN in topN_list:\n",
        "        for excl in excludeK_list:\n",
        "            port_rets = []\n",
        "            per_period_holdings = []\n",
        "            for d in dates_sorted:\n",
        "                nd = next_map[d]\n",
        "                if nd is None: continue\n",
        "\n",
        "                # Get all signals for the current rebalance date, including non-eligible ones.\n",
        "                # Only drop NaNs for 'Score' as these are truly unrankable.\n",
        "                df_d = signals_df[signals_df['RebalanceDate']==d].dropna(subset=['Score']).copy()\n",
        "\n",
        "                if df_d.empty:\n",
        "                    port_rets.append(np.nan); continue\n",
        "\n",
        "                # Create a combined sorting key: prioritize Eligible=True, then by Score (descending).\n",
        "                # A large multiplier (e.g., 1000) ensures eligible stocks are always ranked higher than non-eligible ones\n",
        "                # if their scores are in a reasonable range (e.g., -1 to 1).\n",
        "                df_d['SortKey'] = df_d['Eligible'].astype(int) * 1000 + df_d['Score']\n",
        "\n",
        "                # Sort all stocks by this new SortKey (descending).\n",
        "                df_d_sorted = df_d.sort_values(by='SortKey', ascending=False).reset_index(drop=True)\n",
        "\n",
        "                # Apply exclusion first: remove the top 'excl' stocks from the prioritized list.\n",
        "                df_after_excl = df_d_sorted.iloc[excl:].copy()\n",
        "\n",
        "                # Now, from the remaining stocks, select the top 'topN'.\n",
        "                # This ensures we pick 'topN' stocks if `df_after_excl` has at least 'topN' rows.\n",
        "                # If `df_after_excl` has fewer than 'topN' rows, it will select all available remaining stocks.\n",
        "                sel = df_after_excl.head(topN).copy()\n",
        "\n",
        "                if sel.empty:\n",
        "                    port_rets.append(np.nan); continue\n",
        "\n",
        "                syms = sel['Symbol'].values\n",
        "                # get prices at d and nd\n",
        "                try:\n",
        "                    p_now = price_df.loc[price_df.index <= d].iloc[-1].reindex(syms).values.astype(float)\n",
        "                    p_next = price_df.loc[price_df.index <= nd].iloc[-1].reindex(syms).values.astype(float)\n",
        "                except Exception:\n",
        "                    port_rets.append(np.nan); continue\n",
        "                with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                    rets = (p_next / p_now) - 1.0\n",
        "                weights = np.repeat(1/len(syms), len(syms))\n",
        "                valid = ~np.isnan(rets)\n",
        "                if not np.any(valid):\n",
        "                    port_rets.append(np.nan); continue\n",
        "                w = weights[valid]; w = w / np.sum(w)\n",
        "                port_ret = float(np.nansum(w * rets[valid]))\n",
        "                port_rets.append(port_ret)\n",
        "                # store holdings rows\n",
        "                for i, sym in enumerate(syms):\n",
        "                    per_period_holdings.append({'RebalanceDate': d, 'NextDate': nd, 'Symbol': sym, 'Rank': int(sel.iloc[i]['Rank']) if 'Rank' in sel.columns else np.nan, 'Price_in': float(p_now[i]) if not np.isnan(p_now[i]) else np.nan, 'Price_out': float(p_next[i]) if not np.isnan(p_next[i]) else np.nan, 'Return': float(rets[i]) if not np.isnan(rets[i]) else np.nan, 'Weight': float(weights[i])})\n",
        "            s = pd.Series(port_rets).dropna()\n",
        "            if s.empty:\n",
        "                metrics = {'mean_period':np.nan,'annual_return':np.nan,'annual_vol':np.nan,'sharpe':np.nan,'mdd':np.nan,'obs':0}\n",
        "            else:\n",
        "                meanp = s.mean(); stdp = s.std(ddof=1)\n",
        "                per_year = 12 if freq_label=='monthly' else (26 if freq_label=='fortnightly' else 52)\n",
        "                ann = (1+meanp)**per_year - 1\n",
        "                annvol = stdp * math.sqrt(per_year)\n",
        "                sharpe = ann / annvol if (not np.isnan(annvol) and annvol>0) else np.nan\n",
        "                cum = (1+s).cumprod(); peak = cum.cummax(); dd = (cum-peak)/peak; mdd = dd.min()\n",
        "                metrics = {'mean_period':meanp,'annual_return':ann,'annual_vol':annvol,'sharpe':sharpe,'mdd':mdd,'obs':len(s)}\n",
        "            rows.append({'freq':freq_label,'topN':topN,'excludeK':excl,'weighting':weighting, **metrics})\n",
        "            key = (topN,excl)\n",
        "            if len(per_period_holdings)>0:\n",
        "                holdings_store[key] = pd.DataFrame(per_period_holdings)\n",
        "                periods_store[key] = pd.DataFrame({'RebalanceDate':sorted(set([r['RebalanceDate'] for r in per_period_holdings])), 'PortfolioReturn': np.nan})\n",
        "                # we'll compute per-period returns separately above, but for convenience we retain holdings\n",
        "    results_df = pd.DataFrame(rows)\n",
        "    # save results\n",
        "    out = Path(out_xlsx)\n",
        "    with pd.ExcelWriter(out, engine=\"openpyxl\", datetime_format=\"YYYY-MM-DD\") as writer:\n",
        "        results_df.to_excel(writer, sheet_name=\"ResultsGrid\", index=False)\n",
        "        # top strategies by sharpe\n",
        "        top10 = results_df.sort_values(['sharpe','annual_return'], ascending=[False,False]).head(10)\n",
        "        top10.to_excel(writer, sheet_name=\"Top10_BySharpe\", index=False)\n",
        "        # also write holdings for each top10\n",
        "        for idx, row in top10.iterrows():\n",
        "            key = (int(row['topN']), int(row['excludeK']))\n",
        "            df_hold = holdings_store.get(key)\n",
        "            if df_hold is None:\n",
        "                pd.DataFrame([{\"Note\":\"No holdings for this configuration\"}]).to_excel(writer, sheet_name=f\"H_{row['topN']}_E{row['excludeK']}\"[:31], index=False)\n",
        "            else:\n",
        "                df_hold.to_excel(writer, sheet_name=f\"H_{row['topN']}_E{row['excludeK']}\"[:31], index=False)\n",
        "    print(\"Saved backtest workbook to:\", out)\n",
        "    return out, results_df\n",
        "\n",
        "# -------------------------\n",
        "# Orchestration / UI\n",
        "# -------------------------\n",
        "def main():\n",
        "    print(\"Momentum pipeline interactive runner.\")\n",
        "    print(\"Default ticker list path:\", DEFAULT_INPUT_TICKER_XLSX)\n",
        "    print(\"Outputs are written into:\", OUTPUT_DIR.resolve())\n",
        "    while True:\n",
        "        print(\"\\nMenu:\")\n",
        "        print(\"1) Module 1 — Download data from Yahoo (reads tickers from an Excel file)\")\n",
        "        print(\"2) Module 2 — Clean data (build price matrix & symbol report)\")\n",
        "        print(\"3) Module 3 — Run Momentum (weekly/fortnightly/monthly) -> compute signals\")\n",
        "        print(\"4) Module 4 — Backtest grid & Top10 report (requires cleaned price matrix + signals)\")\n",
        "        print(\"5) Exit\")\n",
        "        choice = input(\"Choose option (1-5): \").strip()\n",
        "        if choice==\"1\":\n",
        "            inpath = input(f\"Ticker list path (press Enter for default {DEFAULT_INPUT_TICKER_XLSX}): \").strip() or DEFAULT_INPUT_TICKER_XLSX\n",
        "            out = input(\"Output XLSX path (press Enter for default './output/yahoo_data.xlsx'): \").strip() or \"./output/yahoo_data.xlsx\"\n",
        "            years_str = input(\"Number of years to download (press Enter for default 2 years): \").strip()\n",
        "            years = int(years_str) if years_str.isdigit() else 2\n",
        "            print(f\"About to download tickers from {inpath} for {years} years and save to {out}\")\n",
        "            if not confirm(\"Confirm run downloader? (y/n): \"): continue\n",
        "            download_yahoo_data(inpath, out, years=years)\n",
        "        elif choice==\"2\":\n",
        "            yahoo_path = input(\"Path to yahoo data XLSX (press Enter for './output/yahoo_data.xlsx'): \").strip() or \"./output/yahoo_data.xlsx\"\n",
        "            if not Path(yahoo_path).exists():\n",
        "                print(\"File not found:\", yahoo_path); continue\n",
        "            out_clean = input(\"Clean output path (press Enter for './output/cleaned_price_matrix.xlsx'): \").strip() or \"./output/cleaned_price_matrix.xlsx\"\n",
        "            print(\"This will build a price matrix and symbol report then save to\", out_clean)\n",
        "            if not confirm(\"Confirm data cleanup? (y/n): \"): continue\n",
        "            clean_data(yahoo_path, out_clean)\n",
        "        elif choice==\"3\":\n",
        "            clean_path = input(\"Path to cleaned price matrix XLSX (press Enter for './output/cleaned_price_matrix.xlsx'): \").strip() or \"./output/cleaned_price_matrix.xlsx\"\n",
        "            if not Path(clean_path).exists():\n",
        "                print(\"Clean file not found:\", clean_path); continue\n",
        "\n",
        "            # Original: sheet_choice = input(\"Which frequency? Enter weekly / fortnightly / monthly: \").strip().lower()\n",
        "            freq_input = input(\"Which frequency? Enter 1 for weekly, 2 for fortnightly, 3 for monthly: \").strip()\n",
        "            if freq_input == '1':\n",
        "                sheet_choice = 'weekly'\n",
        "            elif freq_input == '2':\n",
        "                sheet_choice = 'fortnightly'\n",
        "            elif freq_input == '3':\n",
        "                sheet_choice = 'monthly'\n",
        "            else:\n",
        "                print(\"Invalid choice. Please enter 1, 2, or 3.\"); continue\n",
        "\n",
        "            print(f\"About to compute signals for {sheet_choice}.\")\n",
        "            if not confirm(\"Confirm compute signals? (y/n): \"): continue\n",
        "            # load clean matrix\n",
        "            xl = pd.ExcelFile(clean_path)\n",
        "            if \"CleanPriceMatrix\" in xl.sheet_names:\n",
        "                price_df = pd.read_excel(clean_path, sheet_name=\"CleanPriceMatrix\", parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "            else:\n",
        "                price_df = pd.read_excel(clean_path, sheet_name=xl.sheet_names[0], parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "            rebalance_dates = get_rebalance_dates(price_df.index, freq=sheet_choice)\n",
        "            print(\"Rebalance dates count:\", len(rebalance_dates))\n",
        "            signals_df = compute_scores_and_signals(price_df, rebalance_dates)\n",
        "            sig_out = Path(OUTPUT_DIR)/f\"signals_{sheet_choice}.csv\"\n",
        "            signals_df.to_csv(sig_out, index=False)\n",
        "            print(\"Signals saved to:\", sig_out)\n",
        "        elif choice==\"4\":\n",
        "            price_path = input(\"Path to clean price matrix XLSX (Enter default './output/cleaned_price_matrix.xlsx'): \").strip() or \"./output/cleaned_price_matrix.xlsx\"\n",
        "\n",
        "            # New frequency input for Module 4\n",
        "            freq_input_m4 = input(\"Enter frequency label used for signals (1 for weekly, 2 for fortnightly, 3 for monthly): \").strip()\n",
        "            if freq_input_m4 == '1':\n",
        "                freq_label = 'weekly'\n",
        "            elif freq_input_m4 == '2':\n",
        "                freq_label = 'fortnightly'\n",
        "            elif freq_input_m4 == '3':\n",
        "                freq_label = 'monthly'\n",
        "            else:\n",
        "                print(\"Invalid frequency choice. Please enter 1, 2, or 3.\"); continue\n",
        "\n",
        "            # Updated signals_csv to use the new freq_label\n",
        "            signals_csv = input(f\"Path to signals CSV (press Enter for default './output/signals_{freq_label}.csv'): \").strip() or f\"./output/signals_{freq_label}.csv\"\n",
        "\n",
        "            # Original: freq_label = input(\"Enter frequency label used for signals (weekly/fortnightly/monthly): \").strip().lower()\n",
        "\n",
        "            if not Path(price_path).exists():\n",
        "                print(\"Price file not found:\", price_path); continue\n",
        "            if not Path(signals_csv).exists():\n",
        "                print(\"Signals file not found:\", signals_csv); continue\n",
        "            if freq_label not in (\"weekly\",\"fortnightly\",\"monthly\"):\n",
        "                print(\"Invalid freq\"); continue # This check is redundant with the new input logic but harmless\n",
        "            if not confirm(\"Run backtest and report? (y/n): \"): continue\n",
        "            price_df = pd.read_excel(price_path, sheet_name=\"CleanPriceMatrix\", parse_dates=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "            signals_df = pd.read_csv(signals_csv, parse_dates=[\"RebalanceDate\"])\n",
        "            out_xlsx = Path(OUTPUT_DIR)/f\"results_{freq_label}.xlsx\"\n",
        "            backtest_grid_and_report(price_df, signals_df, sorted(signals_df['RebalanceDate'].unique()), freq_label, out_xlsx=out_xlsx)\n",
        "        elif choice==\"5\":\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Unknown choice. Try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJnlRTiKN62LQdSM3YB/nW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}